{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Sentence Transformer Implementation\n",
    "\n",
    "### Loading Pre-trained Model and Tokenizer\n",
    "In this section, we load the pre-trained model and tokenizer from the `sentence-transformers` library. The `bert-base-nli-mean-tokens` model is specifically designed to generate sentence embeddings.\n",
    "\n",
    "### Defining the Encoding Function\n",
    "Here, we define a function to encode sentences into fixed-length embeddings. The function takes a list of sentences as input, tokenizes them, and then feeds them through the model. The mean of the last hidden states is computed to obtain a single embedding for each sentence.\n",
    "\n",
    "### Sample Sentences and Embedding Display\n",
    "In this section, we test our encoding function with a few sample sentences and display the resulting embeddings. Each embedding is a fixed-length vector representation of the input sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robert/Desktop/MLE/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/robert/Desktop/MLE/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This is a test sentence.\n",
      "Embedding shape: torch.Size([768])\n",
      "Embedding mean: -0.016957160085439682, std: 0.590369462966919\n",
      "\n",
      "Sentence: Sentence transformers are great!\n",
      "Embedding shape: torch.Size([768])\n",
      "Embedding mean: -0.01905854046344757, std: 0.6100025177001953\n",
      "\n",
      "Sentence: This task is interesting.\n",
      "Embedding shape: torch.Size([768])\n",
      "Embedding mean: -0.019545016810297966, std: 0.6200253367424011\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Loading Pre-trained Model and Tokenizer\n",
    "# We use the `sentence-transformers/bert-base-nli-mean-tokens` model to generate sentence embeddings.\n",
    "model_name = 'sentence-transformers/bert-base-nli-mean-tokens'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Defining the Encoding Function\n",
    "# This function encodes input sentences into fixed-length embeddings using mean pooling on the last hidden states.\n",
    "def encode_sentences(sentences, max_length=128):\n",
    "    # Encode input sentences with truncation and padding\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Obtain the last hidden states and compute mean pooling\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "# Sample Sentences and Embedding Display\n",
    "# We test the model with sample sentences and display the resulting embeddings.\n",
    "sentences = [\"This is a test sentence.\", \"Sentence transformers are great!\", \"This task is interesting.\"]\n",
    "embeddings = encode_sentences(sentences)\n",
    "\n",
    "# Check the shape and statistics of the embeddings\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    print(f\"Sentence: {sentences[i]}\")\n",
    "    print(f\"Embedding shape: {embedding.shape}\")\n",
    "    print(f\"Embedding mean: {embedding.mean().item()}, std: {embedding.std().item()}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Multi-Task Learning Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits for Task A (Sentence Classification): tensor([[-0.6985,  0.1159,  0.1417],\n",
      "        [-0.4934, -0.5920, -0.0066],\n",
      "        [-0.4135, -0.3569, -0.2814]], grad_fn=<AddmmBackward0>)\n",
      "Logits for Task B (Sentiment Analysis): tensor([[-0.3619,  0.3586],\n",
      "        [-0.3751,  0.4887],\n",
      "        [-0.3012,  0.4846]], grad_fn=<AddmmBackward0>)\n",
      "Loss for Task A: 1.2000242471694946\n",
      "Loss for Task B: 0.6624807715415955\n",
      "Total Loss: 1.8625049591064453\n",
      "Epoch 1, Loss: 1.9825471639633179\n",
      "Epoch 2, Loss: 1.8293399810791016\n",
      "Epoch 3, Loss: 1.3688188791275024\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Loading Pre-trained Model and Tokenizer\n",
    "model_name = 'sentence-transformers/bert-base-nli-mean-tokens'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Defining Multi-Task Model Architecture\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, base_model, num_classes_task_a, num_classes_task_b):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        # Task A: Sentence Classification Head\n",
    "        self.classifier_task_a = nn.Linear(base_model.config.hidden_size, num_classes_task_a)\n",
    "        # Task B: Sentiment Analysis Head\n",
    "        self.classifier_task_b = nn.Linear(base_model.config.hidden_size, num_classes_task_b)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the [CLS] token representation (first token) as sentence embedding\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        # Task A: Sentence Classification\n",
    "        logits_task_a = self.classifier_task_a(cls_output)\n",
    "        # Task B: Sentiment Analysis\n",
    "        logits_task_b = self.classifier_task_b(cls_output)\n",
    "        return logits_task_a, logits_task_b\n",
    "\n",
    "# Example usage with 3 classes for Task A and 2 classes for Task B\n",
    "num_classes_task_a = 3  # Example: classifying sentences into 3 categories\n",
    "num_classes_task_b = 2  # Example: positive, negative sentiment\n",
    "multi_task_model = MultiTaskModel(base_model, num_classes_task_a, num_classes_task_b)\n",
    "\n",
    "# Sample Input for Testing\n",
    "sentences_task_a = [\"This is a test sentence.\", \"Sentence transformers are great!\", \"This task is interesting.\"]\n",
    "inputs_task_a = tokenizer(sentences_task_a, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "sentences_task_b = [\"I love this movie!\", \"This is the worst day of my life.\", \"I feel fantastic!\"]\n",
    "inputs_task_b = tokenizer(sentences_task_b, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Forward pass for Task A and Task B\n",
    "logits_task_a, _ = multi_task_model(inputs_task_a['input_ids'], inputs_task_a['attention_mask'])\n",
    "_, logits_task_b = multi_task_model(inputs_task_b['input_ids'], inputs_task_b['attention_mask'])\n",
    "\n",
    "# Check the outputs\n",
    "print(f\"Logits for Task A (Sentence Classification): {logits_task_a}\")\n",
    "print(f\"Logits for Task B (Sentiment Analysis): {logits_task_b}\")\n",
    "\n",
    "# Defining loss functions for both tasks\n",
    "criterion_task_a = nn.CrossEntropyLoss()\n",
    "criterion_task_b = nn.CrossEntropyLoss()\n",
    "\n",
    "# Example labels (randomly generated for illustration)\n",
    "labels_task_a = torch.tensor([0, 2, 1])  # Example labels for Task A\n",
    "labels_task_b = torch.tensor([1, 0, 1])  # Example labels for Task B\n",
    "\n",
    "# Calculating losses\n",
    "loss_task_a = criterion_task_a(logits_task_a, labels_task_a)\n",
    "loss_task_b = criterion_task_b(logits_task_b, labels_task_b)\n",
    "total_loss = loss_task_a + loss_task_b\n",
    "\n",
    "print(f\"Loss for Task A: {loss_task_a.item()}\")\n",
    "print(f\"Loss for Task B: {loss_task_b.item()}\")\n",
    "print(f\"Total Loss: {total_loss.item()}\")\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = torch.optim.Adam(multi_task_model.parameters(), lr=1e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    multi_task_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    logits_task_a, _ = multi_task_model(inputs_task_a['input_ids'], inputs_task_a['attention_mask'])\n",
    "    _, logits_task_b = multi_task_model(inputs_task_b['input_ids'], inputs_task_b['attention_mask'])\n",
    "    \n",
    "    loss_task_a = criterion_task_a(logits_task_a, labels_task_a)\n",
    "    loss_task_b = criterion_task_b(logits_task_b, labels_task_b)\n",
    "    total_loss = loss_task_a + loss_task_b\n",
    "    \n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Training Considerations\n",
    "\n",
    "#### Freezing the Entire Network\n",
    "- **Impact and Advantages**: The model will not learn new information and will only use pre-trained knowledge. This approach is beneficial if the pre-trained model already performs well on the new tasks.\n",
    "- **Training Strategy**: No training is needed for the frozen model, just use the pre-trained embeddings directly for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Freeze the entire network\n",
    "for param in multi_task_model.base_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freezing Only the Transformer Backbone\n",
    "- **Impact and Advantages**: This allows the model to retain the pre-trained knowledge while the task-specific heads can learn the new tasks. It reduces training time and computational resources.\n",
    "- **Training Strategy**: Train only the classification heads (`classifier_task_a` and `classifier_task_b`). Use a smaller learning rate for the heads to fine-tune them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2: Freeze only the transformer backbone\n",
    "for param in multi_task_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in multi_task_model.classifier_task_a.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in multi_task_model.classifier_task_b.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freezing Only One Task-Specific Head\n",
    "- **Impact and Advantages**: This enables the model to learn a new task without affecting the other pre-trained task head. Useful when one task is more important or has more data than the other.\n",
    "- **Training Strategy**: Freeze one task-specific head (e.g., `classifier_task_a`) and train the other head (`classifier_task_b`). Adjust the learning rate accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 3: Freeze one task-specific head\n",
    "for param in multi_task_model.classifier_task_a.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in multi_task_model.classifier_task_b.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer Learning Scenario\n",
    "- **Choosing a Pre-trained Model**: Use a model pre-trained on a large, diverse corpus (e.g., `bert-base-nli-mean-tokens`).\n",
    "- **Freezing/Unfreezing Layers**: Freeze the initial layers of the Transformer (to retain basic language understanding) and unfreeze the higher layers and task-specific heads (to fine-tune for the new tasks).\n",
    "- **Rationale**: Freezing the lower layers helps retain general language features, while fine-tuning the higher layers and heads allows the model to adapt to the specific tasks with new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning Scenario\n",
    "for name, param in multi_task_model.base_model.named_parameters():\n",
    "    if \"layer.11\" in name or \"layer.10\" in name:  # Adjust layer numbers based on the specific model\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Ensure task-specific heads are trainable\n",
    "for param in multi_task_model.classifier_task_a.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in multi_task_model.classifier_task_b.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.1768361330032349\n",
      "Epoch 2, Loss: 1.3216972351074219\n",
      "Epoch 3, Loss: 1.0765254497528076\n"
     ]
    }
   ],
   "source": [
    "# Simplified training loop example\n",
    "criterion_task_a = nn.CrossEntropyLoss()\n",
    "criterion_task_b = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, multi_task_model.parameters()), lr=1e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "# Updated input and labels for consistency\n",
    "sentences_task_a = [\"This is a test sentence.\", \"Sentence transformers are great!\", \"This task is interesting.\"]\n",
    "inputs_task_a = tokenizer(sentences_task_a, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "labels_task_a = torch.tensor([0, 2, 1])  # Example labels for Task A\n",
    "\n",
    "sentences_task_b = [\"I love this movie!\", \"This is the worst day of my life.\", \"I feel fantastic!\"]\n",
    "inputs_task_b = tokenizer(sentences_task_b, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "labels_task_b = torch.tensor([1, 0, 1])  # Example labels for Task B\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    multi_task_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass for Task A and Task B\n",
    "    logits_task_a, _ = multi_task_model(inputs_task_a['input_ids'], inputs_task_a['attention_mask'])\n",
    "    _, logits_task_b = multi_task_model(inputs_task_b['input_ids'], inputs_task_b['attention_mask'])\n",
    "    \n",
    "    # Calculate losses for both tasks\n",
    "    loss_task_a = criterion_task_a(logits_task_a, labels_task_a)\n",
    "    loss_task_b = criterion_task_b(logits_task_b, labels_task_b)\n",
    "    total_loss = loss_task_a + loss_task_b\n",
    "    \n",
    "    # Backpropagation\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Summary\n",
    "- **Freezing Entire Network**: No further training, rely on pre-trained knowledge.\n",
    "- **Freezing Transformer Backbone**: Train task-specific heads only.\n",
    "- **Freezing One Task Head**: Train the other task head while retaining the frozen head.\n",
    "- **Transfer Learning**: Freeze lower layers, fine-tune higher layers and task heads, leveraging pre-trained language understanding for specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Layer-wise Learning Rate Implementation (BONUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.1857175827026367\n",
      "Epoch 2, Loss: 0.5991258025169373\n",
      "Epoch 3, Loss: 0.2687854468822479\n"
     ]
    }
   ],
   "source": [
    "# Define layer-wise learning rates\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': multi_task_model.base_model.embeddings.parameters(), 'lr': 1e-5},\n",
    "    {'params': multi_task_model.base_model.encoder.layer[:6].parameters(), 'lr': 5e-5},\n",
    "    {'params': multi_task_model.base_model.encoder.layer[6:].parameters(), 'lr': 1e-4},\n",
    "    {'params': multi_task_model.classifier_task_a.parameters(), 'lr': 1e-3},\n",
    "    {'params': multi_task_model.classifier_task_b.parameters(), 'lr': 1e-3},\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters)\n",
    "\n",
    "# Simplified training loop example\n",
    "criterion_task_a = nn.CrossEntropyLoss()\n",
    "criterion_task_b = nn.CrossEntropyLoss()\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    multi_task_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass for Task A and Task B\n",
    "    logits_task_a, _ = multi_task_model(inputs_task_a['input_ids'], inputs_task_a['attention_mask'])\n",
    "    _, logits_task_b = multi_task_model(inputs_task_b['input_ids'], inputs_task_b['attention_mask'])\n",
    "    \n",
    "    # Calculate losses for both tasks\n",
    "    loss_task_a = criterion_task_a(logits_task_a, labels_task_a)\n",
    "    loss_task_b = criterion_task_b(logits_task_b, labels_task_b)\n",
    "    total_loss = loss_task_a + loss_task_b\n",
    "    \n",
    "    # Backpropagation\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Layer-wise Learning Rates\n",
    "\n",
    "Using different learning rates for different layers helps fine-tune pre-trained models more effectively. Here's our setup:\n",
    "\n",
    "- **Embedding Layer**: We used a very low rate (1e-5) to keep basic token representations stable.\n",
    "- **First 6 Encoder Layers**: These had a slightly higher rate (5e-5) to allow some fine-tuning while preserving general language features.\n",
    "- **Last 6 Encoder Layers**: These needed a higher rate (1e-4) to adapt more to the specific tasks.\n",
    "- **Task-specific Heads**: These new layers had the highest rate (1e-3) to quickly learn task-specific mappings.\n",
    "\n",
    "### Benefits of Layer-wise Learning Rates\n",
    "\n",
    "1. **Fine-tuning Control**:\n",
    "   - Different learning rates let us control how much each part of the model is adjusted, keeping pre-trained knowledge intact while making necessary tweaks.\n",
    "\n",
    "2. **Stability and Convergence**:\n",
    "   - Lower rates for lower layers keep training stable. Higher rates for higher layers, which are more task-specific, help them adapt quickly. This combo helps the model converge faster and more stably.\n",
    "\n",
    "3. **Adaptability**:\n",
    "   - Different parts of the model learning at different rates makes it easier for the network to adapt to new tasks. This flexibility is key for effective fine-tuning.\n",
    "\n",
    "### Impact of Multi-task Setting\n",
    "\n",
    "1. **Enhanced Task-specific Learning**:\n",
    "   - In a multi-task setup, each task might need different adaptations. Layer-wise learning rates let each task-specific head learn efficiently while the shared backbone is fine-tuned in a controlled way.\n",
    "\n",
    "2. **Efficient Resource Utilization**:\n",
    "   - Sharing parameters across tasks can lead to more efficient use of resources. Layer-wise learning rates optimize this, leading to faster training times and better generalization.\n",
    "\n",
    "3. **Mitigation of Catastrophic Forgetting**:\n",
    "   - Learning new tasks can sometimes make a model forget old ones. Using lower rates for shared layers and higher rates for task-specific layers helps the model retain old information while adapting to new tasks, reducing the risk of forgetting.\n",
    "\n",
    "In summary, layer-wise learning rates help fine-tune the model more effectively, keep training stable, and make the model adaptable to multiple tasks without forgetting what it already learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Brief Summary\n",
    "\n",
    "In Task 4, we used layer-wise learning rates to improve the training of a multi-task sentence transformer. Here's what we did:\n",
    "\n",
    "1. **Layer-wise Learning Rates**:\n",
    "   - We applied different learning rates to different layers:\n",
    "     - Embedding Layer: 1e-5\n",
    "     - First 6 Encoder Layers: 5e-5\n",
    "     - Last 6 Encoder Layers: 1e-4\n",
    "     - Task-specific Heads: 1e-3\n",
    "   - This let us fine-tune the model more precisely, keeping the foundational layers stable while letting the task-specific layers adapt more.\n",
    "\n",
    "2. **Fine-tuning Control**:\n",
    "   - Using lower rates for lower layers and higher rates for higher layers and task heads helped fine-tune effectively without losing pre-trained knowledge.\n",
    "\n",
    "3. **Stability and Convergence**:\n",
    "   - This approach made training more stable and led to faster convergence, reducing the risk of overfitting.\n",
    "\n",
    "4. **Adaptability in Multi-task Setting**:\n",
    "   - In a multi-task setup, layer-wise learning rates improved task-specific learning while keeping shared knowledge intact, which is key for efficient training and better generalization.\n",
    "\n",
    "5. **Mitigating Catastrophic Forgetting**:\n",
    "   - Different learning rates helped retain previously learned info while adapting to new tasks, reducing the risk of forgetting.\n",
    "\n",
    "### Results and Conclusion\n",
    "- The loss values dropped significantly over the epochs.\n",
    "- Layer-wise learning rates proved beneficial for fine-tuning, letting the model leverage pre-trained knowledge and adapt to new tasks effectively, leading to better performance and faster convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
